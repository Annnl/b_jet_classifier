{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#The updated main file building b-tag jet origin classifier by DNN and DANN\n",
    "import numpy as np\n",
    "np.random.seed(42) # set the random seed for the reproducibility\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot as ur\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "import time\n",
    "pd.set_option('display.max_columns', None) # to see all columns of df.head()\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from tensorflow.keras.layers import LeakyReLU, PReLU\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import roc_auc_score # for binary classification if x > 0.5 -> 1 else -> 0\n",
    "from sklearn.utils import class_weight # to set class_weight=\"balanced\"\n",
    "from adapt.utils import make_classification_da\n",
    "from adapt.feature_based import DANN\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#when three b-jet under cut, consider the highest score one as additional\n",
    "def compute_4eff_comb(dataset, NNscore,event,n_pt,cut):\n",
    "    copy = event.copy()\n",
    "    event_number = len(set(copy))\n",
    "    bjetpt_model = [[] for _ in range(4)] #lead_top, sub_top, lead_add, sub_add\n",
    "    bjetpt_truth = [[] for _ in range(4)]\n",
    "    n1, n2, n3, n4 = 0,0,0,0\n",
    "    lead_top, sub_top,lead_add, sub_add=0,0,0,0\n",
    "    tot_cut, tot_cor, both_top_cor = 0,0,0\n",
    "    #coll = [[] for _ in range(event_number)]\n",
    "    coll = [[] for _ in range(max(set(copy)))]\n",
    "    for i in range(len(dataset)):\n",
    "        coll[int(event[i])-1].append([NNscore[i], dataset[i], n_pt[i]])\n",
    "        \n",
    "   # for i in range(event_number):\n",
    "    for i in range(max(set(copy))):\n",
    "        score_top = []\n",
    "        index_top = []\n",
    "        index_add = []\n",
    "        score_add = []\n",
    "        score = []\n",
    "        pt = []\n",
    "        index_0 = []\n",
    "        index_1 = []\n",
    "        pt_top =[]\n",
    "        pt_add = []\n",
    "        for j in range(len(coll[i])):\n",
    "            score.append(coll[i][j][0])\n",
    "            pt.append(coll[i][j][2])\n",
    "            if coll[i][j][1] == 0: \n",
    "                pt_top.append(coll[i][j][2])\n",
    "                index_0.append(j)\n",
    "            else: \n",
    "                pt_add.append(coll[i][j][2])\n",
    "                index_1.append(j)\n",
    "            if coll[i][j][0] <= cut: \n",
    "                score_top.append(coll[i][j][0])\n",
    "            else: score_add.append(coll[i][j][0])\n",
    "        # tops by NN algorithm\n",
    "        if len(score_top) == 0:\n",
    "            fir_top_NN = -1\n",
    "            sec_top_NN = -1\n",
    "        elif len(score_top) == 1:\n",
    "            fir_top_NN = score.index(score_top[0])\n",
    "            sec_top_NN = -1\n",
    "            index_top.append(fir_top_NN)\n",
    "        else:\n",
    "            for z in range(2):\n",
    "                mini = min(score_top)\n",
    "                index_top.append(score.index(mini))\n",
    "                score_top.remove(mini)\n",
    "            if len(score_top) > 0:\n",
    "                for z in score_top:\n",
    "                    score_add.append(z)  \n",
    "            if pt[index_top[0]] >= pt[index_top[1]]:\n",
    "                fir_top_NN = index_top[0]\n",
    "                sec_top_NN = index_top[1]\n",
    "            else:\n",
    "                fir_top_NN = index_top[1]\n",
    "                sec_top_NN = index_top[0]\n",
    "       \n",
    "        # tops by truth information\n",
    "        pt_top.sort()\n",
    "        if len(pt_top) > 0: \n",
    "            fir_top_truth = pt.index(pt_top[-1])\n",
    "            bjetpt_truth[0].append(pt_top[-1])\n",
    "        else: fir_top_truth = -1\n",
    "        if len(pt_top) > 1: \n",
    "            sec_top_truth = pt.index(pt_top[-2])\n",
    "            bjetpt_truth[1].append(pt_top[-2])\n",
    "        \n",
    "        \n",
    "        # add by NN algorithm\n",
    "        if len(score_add) == 0:\n",
    "            fir_add_NN = -1\n",
    "            sec_add_NN = -1\n",
    "        elif len(score_add) == 1:\n",
    "            fir_add_NN = score.index(score_add[0])\n",
    "            sec_add_NN = -1\n",
    "            index_add.append(fir_add_NN)\n",
    "        else:\n",
    "            ptadd = []\n",
    "            for z in score_add:\n",
    "                ptadd.append(coll[i][score.index(z)][2])\n",
    "                index_add.append(score.index(z))\n",
    "            ptadd.sort()\n",
    "            fir_add_NN = pt.index(ptadd[-1])\n",
    "            sec_add_NN = pt.index(ptadd[-2])\n",
    "\n",
    "        # add by truth information\n",
    "            \n",
    "        pt_add.sort()\n",
    "        if len(pt_add) > 0: \n",
    "            fir_add_truth = pt.index(pt_add[-1])\n",
    "            bjetpt_truth[2].append(pt_add[-1])\n",
    "        else: fir_add_truth = -1\n",
    "        if len(pt_add) > 1: \n",
    "            sec_add_truth = pt.index(pt_add[-2])\n",
    "            bjetpt_truth[3].append(pt_add[-2])\n",
    "        else: sec_add_truth = -1\n",
    "        \n",
    "        #compute eff of cut\n",
    "        x = len(common_member(index_add, index_1))\n",
    "        y = len(common_member(index_top, index_0))\n",
    "        tot_cut += len(index_1)+len(index_0)\n",
    "        tot_cor += x + y\n",
    "        if y == 2: both_top_cor += 1\n",
    "        \n",
    "        #compare NN and truth\n",
    "        if fir_top_NN != -1:\n",
    "            n1 += 1\n",
    "            bjetpt_model[0].append(pt[fir_top_NN])\n",
    "            if fir_top_NN == fir_top_truth: lead_top += 1\n",
    "        if sec_top_NN != -1:\n",
    "            n2 += 1\n",
    "            bjetpt_model[1].append(pt[sec_top_NN])\n",
    "            if sec_top_NN == sec_top_truth: sub_top += 1\n",
    "        if fir_add_NN != -1:\n",
    "            n3 += 1\n",
    "            bjetpt_model[2].append(pt[fir_add_NN])\n",
    "            if fir_add_NN == fir_add_truth: lead_add += 1\n",
    "        if sec_add_NN != -1:\n",
    "            n4 += 1\n",
    "            bjetpt_model[3].append(pt[sec_add_NN])\n",
    "            if sec_add_NN == sec_add_truth: sub_add += 1\n",
    "    if n4 == 0:eff_sub_add = 0\n",
    "    else:eff_sub_add = sub_add/n4\n",
    "    eff = [lead_top/n1, sub_top/n2,lead_add/n3, eff_sub_add, tot_cor/tot_cut, both_top_cor/event_number]\n",
    "    correct_num = [eff, bjetpt_model, bjetpt_truth]\n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colors = ['r','g']\n",
    "labels = ['truth','All_var model']\n",
    "def find_midpoint(line):\n",
    "    result = [[] for i in range(len(line))]\n",
    "    for i in range(len(line)):\n",
    "        for j in range(len(line[i])-1):\n",
    "            result[i].append((line[i][j+1]+line[i][j])/2)\n",
    "    return result\n",
    "#plot pT bias histograms\n",
    "def check3_pt_bias2(x,coll_all):\n",
    "    bins=[[25,60,90,120,160,400],[25,40,60,400],[25,45,70,100,160,400],[25,35,55,400]]\n",
    "    result = find_midpoint(bins)\n",
    "    name = ['lead b-top','sub-lead b-top','lead b-Add','sub-lead b-Add']\n",
    "    biahist=[coll_all[2][x],coll_all[1][x]]\n",
    "    fig, axs = plt.subplots(2,1, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    (n, bins, patches)=axs[0].hist(biahist, bins[x],color=colors, range =[0,400], \n",
    "                            density=True, histtype='step',label=labels)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel('density')\n",
    "    axs[0].set_yscale('log')\n",
    "    #axs[0].set_xscale('log')\n",
    "    axs[0].set(xticklabels=[])\n",
    "    ratio,div,wdiv = [[] for i in range(len(labels)-1)],[0 for i in range(len(labels)-1)],[0 for i in range(len(labels)-1)]\n",
    "    for i in range(len(n[0])):\n",
    "        for j in range(1):\n",
    "            ratio[j].append(n[j+1][i]/n[0][i])\n",
    "            div[j] += (ratio[j][i]-1)**2\n",
    "            wdiv[j] += ((ratio[j][i]-1)*n[0][i]/(bins[i+1]-bins[i]))**2\n",
    "            axs[1].hlines(y = ratio[j][i], xmin = bins[i], xmax = bins[i+1],color=colors[j+1],label=labels[j+1])\n",
    "    coef = np.polyfit(result[x],ratio[0],1)\n",
    "    poly1d_fn = np.poly1d(coef)\n",
    "    chi2,p_value = stats.chisquare(f_obs=ratio[0], f_exp=poly1d_fn(result[x]))\n",
    "    axs[1].hlines(y = 1, xmin = bins[0], xmax = bins[-1],color = 'r',linestyles='dashed',label='truth')\n",
    "    axs[1].set_xlabel('Pt ('+name[x]+'), GeV')\n",
    "    axs[1].set_ylabel('model / truth')\n",
    "    #axs[1].set_ylim([0.97,1.027])\n",
    "    #axs[1].set_xscale('log')\n",
    "    #axs[1].set_xticks(bins[x], bins[x],fontsize=7)\n",
    "    shownumber = 0\n",
    "    if shownumber == 1:\n",
    "        plt.show()\n",
    "        print(\"deviation of All_var model:\",np.sqrt(div[0]))\n",
    "        print(\"weighted deviation of All_var model:\",1e5*np.sqrt(wdiv[0]))\n",
    "        print(\"slope of pT bias ratio: \",coef[0])\n",
    "        print(\"chi square: \",chi2)\n",
    "    else:\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "        return 1e5*np.sqrt(wdiv[0]),abs(coef[0]),chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#plot loss and accuracy\n",
    "def plot(the_fit,title):\n",
    "    num_epoch = np.arange(1,len(the_fit.history['loss'])+1,1)\n",
    "    plt.plot(num_epoch, the_fit.history['loss'],label=\"train loss\",color='blue')\n",
    "    plt.plot(num_epoch, the_fit.history['val_loss'],label=\"test loss\",color='red')\n",
    "    plt.title(act_f + ', '+str(node)+', schedule lr = '+str(initial_lr)+title+' Model')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "    plt.plot(num_epoch, the_fit.history['accuracy'],label=\"train accuracy\",color='cyan')\n",
    "    plt.plot(num_epoch, the_fit.history['val_accuracy'],label=\"test accuracy\",color='orange')\n",
    "    plt.title(act_f + ', '+str(node)+', schedule lr = '+str(initial_lr)+title+' Model')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "    print(\"the last 5 average accuracy: \",np.mean(the_fit.history['val_accuracy'][-5:]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     5,
     16
    ]
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "def common_member(a, b):\n",
    "    result = [i for i in a if i in b]\n",
    "    return result\n",
    "def amsasimov(s,b):\n",
    "        if b<=0 or s<=0:\n",
    "            return 0\n",
    "        try:\n",
    "            return sqrt(2*((s+b)*log(1+float(s)/b)-s))\n",
    "        except ValueError:\n",
    "            print(1+float(s)/b)\n",
    "            print (2*((s+b)*log(1+float(s)/b)-s))\n",
    "        #return s/sqrt(s+b)\n",
    "\n",
    "def compare_train_test(\n",
    "    y_pred_train, y_train, y_pred, y_test, high_low=(0,1), \n",
    "    bins=30,xlabel=\"\", ylabel=\"Arbitrary units\", title=\"\", \n",
    "    weights_train=np.array([]), weights_test=np.array([]),\n",
    "    density=True):\n",
    "    \n",
    "    if weights_train.size != 0:\n",
    "        weights_train_signal = weights_train[y_train == 1]\n",
    "        weights_train_background = weights_train[y_train == 0]\n",
    "    else:\n",
    "        weights_train_signal = None\n",
    "        weights_train_background = None\n",
    "    plt.hist(y_pred_train[y_train == 1],\n",
    "                 color='r', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', density=density,\n",
    "                 label='S (train)', weights=weights_train_signal) # alpha is transparancy\n",
    "    plt.hist(y_pred_train[y_train == 0],\n",
    "                 color='b', alpha=0.5, range=high_low, bins=bins,\n",
    "                 histtype='stepfilled', density=density,\n",
    "                 label='B (train)', weights=weights_train_background)\n",
    "\n",
    "    if weights_test.size != 0:\n",
    "        weights_test_signal = weights_test[y_test == 1]\n",
    "        weights_test_background = weights_test[y_test == 0]\n",
    "    else:\n",
    "        weights_test_signal = None\n",
    "        weights_test_background = None\n",
    "    hist, bins = np.histogram(y_pred[y_test == 1],\n",
    "                                  bins=bins, range=high_low, density=density, weights=weights_test_signal)\n",
    "    scale = len(y_pred[y_test == 1]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='S (test)')\n",
    "\n",
    "    hist, bins = np.histogram(y_pred[y_test == 0],\n",
    "                                  bins=bins, range=high_low, density=density, weights=weights_test_background)\n",
    "    scale = len(y_pred[y_test == 0]) / sum(hist)\n",
    "    err = np.sqrt(hist * scale) / scale\n",
    "\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "    plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='B (test)')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# remove variables not used in training\n",
    "def extract(fulldata):\n",
    "    weights = fulldata[\"total_event_weight\"]\n",
    "    del fulldata[\"total_event_weight\"]\n",
    "    target = fulldata[\"jet_GBHInit_topHadronOriginFlag\"]\n",
    "    del fulldata[\"jet_GBHInit_topHadronOriginFlag\"]\n",
    "    bjet_num = fulldata['bjet_number']\n",
    "    del fulldata['bjet_number']\n",
    "    event = fulldata[\"Event_number\"]\n",
    "    del fulldata[\"Event_number\"]\n",
    "    pT = fulldata[ 'bjet_pt']\n",
    "   # del fulldata[ 'bjet_pt']\n",
    "    \n",
    "    del fulldata[ 'met_phi']\n",
    "    del fulldata[ 'bjet_phi']\n",
    "    del fulldata[ 'el_phi']\n",
    "    del fulldata[ 'mu_phi']\n",
    "    del fulldata[ 'bjet_eta']\n",
    "    del fulldata[ 'el_eta']\n",
    "    del fulldata[ 'mu_eta']\n",
    "    del fulldata[ 'bjet_E']\n",
    "    del fulldata[ 'el_E']\n",
    "    del fulldata[ 'mu_E']\n",
    "    del fulldata[ 'met_pt']\n",
    "    del fulldata[ 'el_pt']\n",
    "    del fulldata[ 'mu_pt']\n",
    "    \n",
    "    del fulldata[  'dR_unb_close']\n",
    "    del fulldata[  'dR_unb_far'] \n",
    "    target = list(target)\n",
    "    for i in range(len(target)):\n",
    "        if target[i] == 4:target[i] = 0\n",
    "        else:target[i] = 1\n",
    "    return weights,pd.Series(target),bjet_num,event,pT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# set up zero for some features for DANN\n",
    "def set_zero(fulldata):\n",
    "    fulldata2 = fulldata.copy()\n",
    "    fulldata3 = fulldata.copy()\n",
    "    fulldata3.loc[:,'bjet_pt'] = 0\n",
    "    fulldata2.loc[:,'mass_bjet_large'] = 0\n",
    "    fulldata2.loc[:,'mass_bjet_small'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_far'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_close'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_el'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_mu'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_1pt'] = 0\n",
    "    fulldata2.loc[:,'mass_lep_2pt'] = 0\n",
    "    return fulldata3,fulldata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# delete useless features\n",
    "def drop_var(fulldata):\n",
    "  #  del fulldata[\"dR_bjet_far\"]\n",
    "    del fulldata2['mass_bjet_large']\n",
    "    fulldata2 = fulldata.copy()\n",
    "  # fulldata3 = fulldata.copy()\n",
    "\n",
    "   # del fulldata3[\"dR_bjet_close\"]\n",
    "    del fulldata2['mass_bjet_small']\n",
    " \n",
    "  #  del fulldata3['dR_lep_far']\n",
    "   # del fulldata3['dR_lep_close']\n",
    "    del fulldata2['mass_lep_far']\n",
    "    del fulldata2['mass_lep_close']\n",
    "   # del fulldata3['dR_lep_el']\n",
    "   # del fulldata3['dR_lep_mu']\n",
    "    del fulldata2['mass_lep_el']\n",
    "    del fulldata2['mass_lep_mu']\n",
    "  #  del fulldata3['dR_lep_1pt']\n",
    "  #  del fulldata3['dR_lep_2pt']\n",
    "    del fulldata2['mass_lep_1pt']\n",
    "    del fulldata2['mass_lep_2pt']\n",
    "    del fulldata['bjet_pt']\n",
    "    return fulldata,fulldata2#,fulldata3\n",
    "#del fulldata[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename=(\"3j3b.root\")\n",
    "file = ur.open(filename)\n",
    "print(file.classnames())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = file[\"ttbar\"]\n",
    "print(type(tree))\n",
    "tree.show()\n",
    "fulldata = tree.arrays(library=\"pd\")\n",
    "\n",
    "#shuffle the events [already done but just to be safe]\n",
    "fulldata = fulldata.sample(frac=1).reset_index(drop=True)\n",
    "print (\"File loaded with \",fulldata.shape[0], \" events \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_input = 16\n",
    "correlation = np.zeros((num_input+1, num_input+1))\n",
    "for i in range(num_input+1):\n",
    "    for j in range(num_input+1):\n",
    "        ii = i\n",
    "        jj = j\n",
    "        if i == num_input: ii += 1\n",
    "        if j == num_input: jj += 1\n",
    "        x = fulldata[fulldata.columns[ii]].to_numpy()\n",
    "        y = fulldata[fulldata.columns[jj]].to_numpy()\n",
    "        correlation[i][j] = int(pearsonr(x,y)[0]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "min_val, max_val = 0, num_input+1\n",
    "intersection_matrix = correlation\n",
    "ax.matshow(correlation, cmap=plt.cm.Blues)\n",
    "\n",
    "for i in range(num_input+1):\n",
    "    for j in range(num_input+1):\n",
    "        c = int(correlation[j,i])\n",
    "        ax.text(i, j, str(c), va='center', ha='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fulldata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Pandas Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#examine first few events\n",
    "fulldata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make a scan over scale factor of DANN\n",
    "def scan_lambda(lamda,X_train_class,X_train_domain,X_test_class,X_test_domain, \n",
    "                y_train, y_test, num_train, num_test, pt_train, pt_test):\n",
    "    auc,bias,slope,chi2 = [],[],[],[]\n",
    "    model, the_fit = train_DANN(lamda)\n",
    "    y_pred_test_class = model.predict(X_test_class).ravel()\n",
    "    y_pred_train_class = model.predict(X_train_class).ravel()\n",
    "    y_pred_test_domain = model.predict(X_test_domain).ravel()\n",
    "    y_pred_train_domain = model.predict(X_train_domain).ravel()\n",
    "    n_num = pd.concat([num_test.copy(),num_train.copy()],ignore_index = True)\n",
    "    n_score_class = np.concatenate((y_pred_test_class.copy(),y_pred_train_class.copy()))\n",
    "    n_score_domain = np.concatenate((y_pred_test_domain.copy(),y_pred_train_domain.copy()))\n",
    "    n_truth = pd.concat([y_test.copy(),y_train.copy()],ignore_index = True)\n",
    "    n_pt = pd.concat([pt_test.copy(),pt_train.copy()],ignore_index = True)\n",
    "    auc_test_class = roc_auc_score(y_true=y_test, y_score=y_pred_test_class)#,sample_weight=weights_test)\n",
    "    auc_test_domain = roc_auc_score(y_true=y_test, y_score=y_pred_test_domain)#,sample_weight=weights_test)\n",
    "    auc_train_class = roc_auc_score(y_true=y_train.values, y_score=y_pred_train_class)#,sample_weight=weights_train)\n",
    "    auc_train_domain = roc_auc_score(y_true=y_train.values, y_score=y_pred_train_domain)#,sample_weight=weights_train)\n",
    "    auc = [auc_test_class,auc_test_domain,auc_train_class,auc_train_domain]\n",
    "    coll_all_class = compute_4eff_comb(n_truth, n_score_class, n_num, n_pt, 1)\n",
    "    #coll_all_domain = compute_4eff_comb(n_truth, n_score_domain, n_num, n_pt, 1)\n",
    "    if target_eff == 4: plot_range = [0,1,2,3]\n",
    "    else:plot_range = [0,1,2]\n",
    "    for i in plot_range:\n",
    "        x,y,z = check3_pt_bias2(i,coll_all_class)\n",
    "        bias.append(x)\n",
    "        slope.append(y)\n",
    "        chi2.append(z)\n",
    "    return auc,bias,slope,chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print('all_var model: ',fulldata.shape)\n",
    "target_eff = 4\n",
    "if target_eff == 4: \n",
    "    fulldata = fulldata[fulldata.bjet_number>3]  \n",
    "else:   fulldata = fulldata[fulldata.bjet_number==3]  \n",
    "weights,target,bjet_num,event,pt = extract(fulldata)\n",
    "#fulldata2,fulldata3 = drop_var(fulldata)\n",
    "#coll_full = [fulldata,fulldata2,fulldata3]\n",
    "print('fulldata: ',fulldata.shape)\n",
    "#print('dR model: ',fulldata2.shape)\n",
    "#print('Mass model: ',fulldata3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     9,
     44
    ]
   },
   "outputs": [],
   "source": [
    "def lr_exp_decay(epoch):\n",
    "    k = 0.1\n",
    "    return initial_lr * tf.math.exp(-k*epoch)\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
    "    return lr\n",
    "\n",
    "#class SensitivitySpecificityCallback(tf.keras.callbacks.Callback):\n",
    "#    def on_epoch_end(self, epoch, logs=None):\n",
    "#        [train_loss[epoch],train_acc[epoch],train_lr[epoch]] = self.model.evaluate(self.X_train,self.y_train)\n",
    "        #,sample_weight=weights_train)\n",
    "\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0,patience=5,\n",
    "                                                 restore_best_weights=True) \n",
    "def build_model(X_train,node):\n",
    "    leaky_relu = LeakyReLU(alpha=0.01)\n",
    "    para_relu = PReLU()    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "    for i in range(len(node)):\n",
    "        model.add(layers.Dense(node[i], activation= act_f))\n",
    "        #layers.Dropout(0.5)\n",
    "    model.add(layers.Dense(1, activation= act_f))\n",
    "    #layers.BatchNormalization()\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = initial_lr)\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=optimizer,\n",
    "              metrics = ['accuracy',lr_metric])\n",
    "    return model\n",
    "def build_DANN_model(node):\n",
    "    leaky_relu = LeakyReLU(alpha=0.01)\n",
    "    para_relu = PReLU()    \n",
    "    model = tf.keras.models.Sequential()\n",
    "   # model.add(layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "    for i in range(len(node)):\n",
    "        model.add(layers.Dense(node[i], activation= act_f))\n",
    "        #layers.Dropout(0.5)\n",
    "    #layers.BatchNormalization()\n",
    "    return model\n",
    "def NNmodel(No_model):\n",
    "    train_size = 0.75 # fraction of sample used for training\n",
    "\n",
    "    X_train, X_test, y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test= \\\n",
    "        train_test_split(coll_full[No_model], target, weights, event, pT, train_size=train_size)\n",
    "    \n",
    "    y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test = \\\n",
    "        y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "        weights_train.reset_index(drop=True), weights_test.reset_index(drop=True),\\\n",
    "        num_train.reset_index(drop=True),num_test.reset_index(drop=True),\\\n",
    "        pt_train.reset_index(drop=True),pt_test.reset_index(drop=True)\n",
    "\n",
    "    #X_test, X_val, y_test, y_val, weights_test, weights_val, = \\\n",
    "    #    train_test_split(X_test, y_test, weights_test, train_size=0.5, shuffle=False)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    #X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test) #applies the transformation calculated the line above\n",
    "\n",
    "    class_weights_train = (weights_train[y_train == 0].sum(), weights_train[y_train == 1].sum())\n",
    "    print (\"class_weights_train:\",class_weights_train)\n",
    "    for i in range(len(class_weights_train)):\n",
    "        weights_train[y_train == i] *= max(class_weights_train)/ class_weights_train[i] #equalize number of background and signal event\n",
    "        weights_test[y_test == i] *= 1/(1-train_size) #increase test weight to compensate for sampling\n",
    "    \n",
    "    print (\"Train : total weight sig\", weights_train[y_train == 1].sum())\n",
    "    print (\"Train : total weight bkg\", weights_train[y_train == 0].sum())\n",
    "    print (\"Test : total weight sig\", weights_test[y_test == 1].sum())\n",
    "    print (\"Test : total weight bkg\", weights_test[y_test == 0].sum())\n",
    "\n",
    "  #  train_loss,train_acc,train_lr = [0 for _ in range(num_epoch)],[0 for _ in range(num_epoch)],[0 for _ in range(num_epoch)]\n",
    "   \n",
    "    starting_time = time.time( )\n",
    "    model = build_model(X_train)\n",
    "    the_fit = model.fit(X_train, y_train.values, epochs=num_epoch,\n",
    "                     validation_data=(X_test,y_test),#weights_test),\n",
    "                    #sample_weight=weights_train,\n",
    "                  #  batch_size = size,\n",
    "                    callbacks=[#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay),earlyStopping])\n",
    "\n",
    "    training_time = time.time( ) - starting_time\n",
    "    print(\"Training time:\",training_time)\n",
    "    y_pred_test = model.predict(X_test).ravel()\n",
    "    y_pred_train = model.predict(X_train).ravel()\n",
    "    n_num = pd.concat([num_test.copy(),num_train.copy()],ignore_index = True)\n",
    "    n_score = np.concatenate((y_pred_test.copy(),y_pred_train.copy()))\n",
    "    n_truth = pd.concat([y_test.copy(),y_train.copy()],ignore_index = True)\n",
    "    n_pt = pd.concat([pt_test.copy(),pt_train.copy()],ignore_index = True)\n",
    "\n",
    "    compare_train_test(y_pred_train, y_train, y_pred_test, y_test, \n",
    "                   xlabel=\"NN Score\", title=\"weighted NN All_var model\",\n",
    "                   weights_train=weights_train.values,\n",
    "                   weights_test=weights_test.values)\n",
    "    return the_fit,[n_truth,n_score,n_num,n_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def trans(X_train,X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "train_size = 0.75 # fraction of sample used for training\n",
    "\n",
    "X_train, X_test, y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test= \\\n",
    "        train_test_split(fulldata, target, weights, event, pt, train_size=train_size)\n",
    "\n",
    "y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test = \\\n",
    "        y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "        weights_train.reset_index(drop=True), weights_test.reset_index(drop=True),\\\n",
    "        num_train.reset_index(drop=True),num_test.reset_index(drop=True),\\\n",
    "        pt_train.reset_index(drop=True),pt_test.reset_index(drop=True)\n",
    "    #X_test, X_val, y_test, y_val, weights_test, weights_val, = \\\n",
    "    #    train_test_split(X_test, y_test, weights_test, train_size=0.5, shuffle=False)\n",
    "X_train_class,X_train_domain = set_zero(X_train)\n",
    "X_test_class,X_test_domain = set_zero(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class,X_test_class = trans(X_train_class,X_test_class)\n",
    "X_train_domain,X_test_domain = trans(X_train_domain,X_test_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0.5\n",
    "num_epoch = 1\n",
    "act_f = 'sigmoid'\n",
    "initial_lr = 0.003\n",
    "node = [128 for i in range(2)]\n",
    "batch_size = 64\n",
    "num_classes = 1\n",
    "#node = [128 ,64 ,32, 16, 8, 4]\n",
    "#node = [8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6,
     23
    ]
   },
   "outputs": [],
   "source": [
    "  #  train_loss,train_acc,train_lr = [0 for _ in range(num_epoch)],[0 for _ in range(num_epoch)],[0 for _ in range(num_epoch)]\n",
    "starting_time = time.time( )\n",
    "phase = 'dann'\n",
    "#phase = 'regular'\n",
    "import collections\n",
    "  #  X_train_class, y_train, X_train_domain,y_train = make_classification_da()\n",
    "# train DANN with only one branch\n",
    "def train_DANN(lamda):\n",
    "    encoder = build_DANN_model(node)\n",
    "    task = build_DANN_model([1])\n",
    "    discriminator = build_DANN_model([128,1])\n",
    "    model = DANN(encoder, task, discriminator, lambda_=lamda, random_state=0)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = initial_lr)\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=optimizer,\n",
    "              metrics = ['accuracy',lr_metric])\n",
    "    the_fit = model.fit(X_train0, y_train0.values,X_train1, y_train1.values, epochs=num_epoch,verbose=0,\n",
    "                     validation_data=(X_test0,y_test0),#weights_test),\n",
    "                    #sample_weight=weights_train,\n",
    "                  #  batch_size = size,\n",
    "                    callbacks=[#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay)])\n",
    "    return model, the_fit\n",
    "# train DANN with only multiple branches (failed)\n",
    "def train_DANN2(lamda):\n",
    "    encoder = build_DANN_model(node)\n",
    "    task = build_DANN_model([1])\n",
    "    discriminator1 = build_DANN_model([128,1])\n",
    "    discriminator2 = build_DANN_model([128,1])\n",
    "    discriminator3 = build_DANN_model([128,1])\n",
    "    model = DANN2(encoder, task, discriminator1, discriminator2, discriminator3, lambda_=lamda, random_state=0)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate = initial_lr)\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "    model.compile(loss=\"binary_crossentropy\", \n",
    "              optimizer=optimizer,\n",
    "              metrics = ['accuracy',lr_metric])\n",
    "    the_fit = model.fit(X_train0, y_train0.values,X_train1, y_train1.values, epochs=num_epoch,verbose=0,\n",
    "                     validation_data=(X_test0,y_test0),#weights_test),\n",
    "                    #sample_weight=weights_train,\n",
    "                  #  batch_size = size,\n",
    "                    callbacks=[#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay)])\n",
    "    return model, the_fit\n",
    "def train_DANN_Egor(lamda,X_train0, X_test0,X_train1, X_test1, y_train0, y_test0, y_train1, y_test1):\n",
    "    label_preds_head_name = 'label_preds'\n",
    "    domain1_preds_head_name = 'domain1_preds'\n",
    "    inputs        = tf.keras.layers.Input(shape=(X_train0.shape[1],))\n",
    "    hidden1       = tf.keras.layers.Dense(30, activation='relu')(inputs)\n",
    "    hidden2       = tf.keras.layers.Dense(15, activation='relu')(hidden1)\n",
    "    hidden3       = tf.keras.layers.Dense(7, activation='relu')(hidden2)\n",
    "    label_preds   = tf.keras.layers.Dense(num_classes, activation='softmax', name=label_preds_head_name)(hidden3)\n",
    "\n",
    "    label_prediction_model = tf.keras.models.Model(\n",
    "        inputs  = inputs,\n",
    "        outputs = label_preds,\n",
    "        name    = 'classification_model')\n",
    "    label_prediction_model.summary()\n",
    "    \n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    "    lr_metric = get_lr_metric(optimizer)\n",
    "    label_prediction_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                               loss='binary_crossentropy',\n",
    "                               metrics=['AUC',lr_metric])\n",
    "\n",
    "    nom_fit = label_prediction_model.fit(x = X_train0,\n",
    "                                     y = y_train0,\n",
    "                                     epochs = num_epoch,\n",
    "                                     validation_data = (X_test0, y_test0),\n",
    "                                     callbacks = [#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay)])\n",
    "\n",
    "    d1_as_nom_fit = label_prediction_model.fit(x = X_train1,\n",
    "                                           y = y_train1,\n",
    "                                           epochs = num_epoch,\n",
    "                                           validation_data = (X_test1, y_test1),\n",
    "                                           callbacks = [#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay)])\n",
    "\n",
    "\n",
    "    @tf.custom_gradient\n",
    "    def reverse_gradient(x, hp_lambda):\n",
    "        # Feed-forward operation:\n",
    "        y = tf.identity(x)\n",
    "        # Back-propagation/gradient-computing operation:\n",
    "        def _flip_gradient(dy):\n",
    "            # Since the decorated function 'reverse_gradient()' actually has 2 inputs\n",
    "            # (counting 'hp_lambda'), we have to return the gradient for each -- but\n",
    "            # anyway, the derivative wrt 'hp_lambda' is null:\n",
    "            return tf.math.negative(dy) * hp_lambda, tf.constant(0.)\n",
    "        return y, _flip_gradient\n",
    "\n",
    "\n",
    "    # wrap the reverse gradient as a Keras layer\n",
    "    class GradientReversal (tf.keras.layers.Layer):\n",
    "\n",
    "        def __init__(self, hp_lambda, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.hp_lambda = hp_lambda\n",
    "\n",
    "        def call(self, inputs, training=None):\n",
    "            return reverse_gradient(inputs, self.hp_lambda)\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config['hp_lambda'] = self.hp_lambda\n",
    "            return config\n",
    "\n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            return cls(**config)\n",
    "\n",
    "\n",
    "    # create domain classification model\n",
    "    hp_lambda = tf.Variable(lamda)\n",
    "    num_domains = 1 # 2: 'source' vs. 'target' (maybe not what we need), 1: prob. to be more or less 'source'-like\n",
    "\n",
    "    domain1_preds_head_name = 'domain1_preds'\n",
    "    domain1_preds = GradientReversal(hp_lambda)(hidden3)\n",
    "    print(\"check: \",domain1_preds)\n",
    "    domain1_preds = tf.keras.layers.Dense(12, activation='linear')(domain1_preds)\n",
    "    domain1_preds = tf.keras.layers.Dense(5, activation='linear', name=\"do5\")(domain1_preds)\n",
    "    domain1_preds = tf.keras.layers.Activation(\"elu\", name=\"do6\")(domain1_preds)\n",
    "    domain1_preds = tf.keras.layers.Dropout(0.5)(domain1_preds)\n",
    "    domain1_preds = tf.keras.layers.Dense(num_domains, activation='softmax', name=domain1_preds_head_name)(domain1_preds)\n",
    "    domain_classification_model1 = tf.keras.models.Model(inputs=inputs, outputs=domain1_preds)\n",
    "\n",
    "    # build a combined model\n",
    "    combined_model = tf.keras.models.Model(inputs=inputs, outputs=[label_preds, domain1_preds])\n",
    "\n",
    "    combined_model.compile(\n",
    "                       optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss={\n",
    "                           label_preds_head_name:   'binary_crossentropy',\n",
    "                           domain1_preds_head_name: 'binary_crossentropy'},\n",
    "                       loss_weights={\n",
    "                           label_preds_head_name:   1,\n",
    "                           domain1_preds_head_name: 1},\n",
    "                       metrics={ # weighted_metrics ?\n",
    "                           label_preds_head_name:   ['AUC',lr_metric],\n",
    "                           domain1_preds_head_name: ['AUC',lr_metric]}\n",
    "                       )\n",
    "\n",
    "    combined_model.summary()\n",
    "    \n",
    "    # training dataset\n",
    "    nominal_dataset = tf.data.Dataset.from_tensor_slices((X_train0, y_train0))\n",
    "    nominal_dataset = nominal_dataset.batch(int(batch_size/2))\n",
    "    #domain1_dataset = tf.data.Dataset.from_tensor_slices(x_target_for_train)\n",
    "    domain1_dataset = tf.data.Dataset.from_tensor_slices((X_train1, y_train1))\n",
    "    domain1_dataset = domain1_dataset.batch(int(batch_size/2))\n",
    "    training_dataset = tf.data.Dataset.zip((nominal_dataset, domain1_dataset))\n",
    "\n",
    "    # 'images' - coordinates in the phase spase.\n",
    "    def _prepare_data_for_dann_training(nominal_data, domain1_data,\n",
    "                                    main_head_name='main_preds', domain1_head_name='domain_preds'):\n",
    "\n",
    "        nominal_images, nominal_labels = nominal_data\n",
    "        domain1_images, domain1_labels = domain1_data\n",
    "\n",
    "        num_nominal = tf.shape(nominal_images)[0]\n",
    "        num_domain1 = tf.shape(domain1_images)[0]\n",
    "\n",
    "        batch_images = tf.concat((nominal_images, domain1_images), axis=0)\n",
    "        batch_labels = tf.concat((nominal_labels, domain1_labels), axis=0)\n",
    "        batch_labels = tf.cast(batch_labels, dtype=tf.float32)\n",
    "    \n",
    "        # not to penalize the model for its prediscions on the domain images,\n",
    "        # by assigning a weight = 0 to these elements of the batch:\n",
    "        nominal_weight_per_sample = tf.tile([1.], [num_nominal])\n",
    "        domain1_weight_per_sample = tf.tile([0.], [num_domain1])\n",
    "        batch_sample_weights = tf.concat((nominal_weight_per_sample, domain1_weight_per_sample), axis=0)\n",
    "    \n",
    "        # domain classifiscation\n",
    "        # we prepared ydn_train and yd1_train to be as what we need here\n",
    "        # but it is simpler to reuse batch_sample_weights that passing extra argument to the function\n",
    "        domain1_labels = batch_sample_weights\n",
    "        domain1_sample_weights = tf.tile([1.], [num_nominal + num_domain1])\n",
    "\n",
    "        batch_domain1 = {main_head_name: batch_labels,\n",
    "                         domain1_head_name: domain1_labels}\n",
    "        batch_sample_weights = {main_head_name: batch_sample_weights,\n",
    "                            domain1_head_name: domain1_sample_weights}\n",
    "\n",
    "        return batch_images, batch_domain1, batch_sample_weights\n",
    "\n",
    "\n",
    "    import functools\n",
    "\n",
    "\n",
    "    prepare_for_dann_training_fn = functools.partial(_prepare_data_for_dann_training,\n",
    "                                                 main_head_name=label_preds_head_name,\n",
    "                                                 domain1_head_name=domain1_preds_head_name)\n",
    "\n",
    "    training_dataset = training_dataset.map(prepare_for_dann_training_fn, num_parallel_calls=4)\n",
    "    print(\"training_dataset:\",training_dataset)\n",
    "\n",
    "\n",
    "    # testing dataset\n",
    "    testing_dataset = tf.data.Dataset.from_tensor_slices((X_test1, y_test1))\n",
    "    testing_dataset = testing_dataset.batch(batch_size)\n",
    "\n",
    "    def _prepare_data_for_dann_testing(domain1_images, domain1_labels,\n",
    "                                      main_head_name='main_preds', domain1_head_name='domain_preds'):\n",
    "        # the batch contains only validation/test images from the target domain. \n",
    "        # this time, we want to evaluate the main loss over these images, so we assign a normal loss\n",
    "        # weight = 1 to each samples.\n",
    "        num_samples = tf.shape(domain1_images)[0]\n",
    "\n",
    "        # want to evaluate over\n",
    "        loss_weights = tf.tile([1], [num_samples])\n",
    "        loss_weights = tf.cast(loss_weights, dtype=tf.float32)\n",
    "        # to assure we have labels as zeroes\n",
    "        domain1_labels = tf.tile([0], [num_samples])\n",
    "        domain1_labels = tf.cast(domain1_labels, dtype=tf.float32)\n",
    "        batch_targets = {main_head_name: domain1_labels,\n",
    "                     domain1_head_name: domain1_labels}\n",
    "        batch_sample_weights = {main_head_name: loss_weights,\n",
    "                            domain1_head_name: loss_weights}\n",
    "\n",
    "        return domain1_images, batch_targets, batch_sample_weights\n",
    "\n",
    "\n",
    "    prepare_for_dann_testing_fn = functools.partial(_prepare_data_for_dann_testing,\n",
    "                                                main_head_name=label_preds_head_name,\n",
    "                                                domain1_head_name=domain1_preds_head_name)\n",
    "\n",
    "    testing_dataset = testing_dataset.map(prepare_for_dann_testing_fn, num_parallel_calls=4)\n",
    "    print(\"testing_dataset:\",testing_dataset)\n",
    "    \n",
    "    metrics_to_print = collections.OrderedDict([\n",
    "        # (\"comb-loss\", \"loss\"), \n",
    "        (\"c-loss\", label_preds_head_name + \"_loss\"),\n",
    "        (\"d-loss\", domain1_preds_head_name + \"_loss\"), \n",
    "        (\"c-acc\", label_preds_head_name + \"_acc\"),\n",
    "        (\"d-acc\", domain1_preds_head_name + \"_acc\"),\n",
    "        (\"target c-acc\", \"val_\" + label_preds_head_name + \"_acc\"),\n",
    "        # (\"target d-acc\", \"val_\" + domain_preds_head_name + \"_acc\")\n",
    "    ])\n",
    "    from keras_custom_callbacks import SimpleLogCallback\n",
    "    callbacks = [\n",
    "        # Callback to simply log metrics at the end of each epoch (saving space compared to verbose=1/2):\n",
    "        SimpleLogCallback(metrics_to_print, num_epochs=num_epoch, log_frequency=1)]\n",
    "\n",
    "    history = combined_model.fit(\n",
    "        training_dataset, epochs=num_epoch, #steps_per_epoch=train_steps_per_epoch,\n",
    "        #validation_data=testing_dataset, #validation_steps=test_steps_per_epoch,\n",
    "         callbacks=callbacks)\n",
    "    return label_prediction_model,combined_model,nom_fit,d1_as_nom_fit,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "X_train0, X_test0,X_train3, X_test3, y_train0, y_test0, num_train0, num_test0, pt_train0, pt_test0=analyze_file_step1(\"3j3b.root\")\n",
    "X_train1, X_test1, y_train1, y_test1, num_train1, num_test1, pt_train1, pt_test1=analyze_file_step1(\"3j3b_sherpa.root\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_prediction_model,combined_model,nom_fit,d1_as_nom_fit,history=train_DANN_Egor(0,X_train0, X_test0,X_train3, X_test3, y_train0, y_test0, y_train0, y_test0)\n",
    "#label_prediction_model,combined_model,nom_fit,d1_as_nom_fit,history=train_DANN_Egor(0,x_source, x_target,x_target, x_source, y_source, y_target, y_target, y_source)\n",
    "# evaluate the model\n",
    "loss_n, acc_n = model.evaluate(X_test0, y_test0.values, weighted_metrics=None)\n",
    "print('Measured model accuracy on the nominal dataset: {}, loss: {}'.format(acc_n, loss_n))\n",
    "loss_d1, acc_d1 = model.evaluate(X_test3, y_test0, weighted_metrics=None)\n",
    "print('Measured model accuracy on the dom1 dataset: {}, loss: {}'.format(acc_d1, loss_d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     25,
     61,
     73,
     107,
     176,
     198
    ]
   },
   "outputs": [],
   "source": [
    "def lr_time_based_decay(epoch, lr):\n",
    "    learning_rate = lr * 1 / (1 + decay * epoch)\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "def lr_step_decay(epoch, lr):\n",
    "    drop_rate = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    learning_rate = initial_learning_rate * math.pow(drop_rate, math.floor(epoch/epochs_drop))\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    learning_rate = initial_learning_rate * math.exp(-k*epoch)\n",
    "    tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "X_train, X_test, y_train, y_test = X_train0, X_test0, y_train0, y_test0\n",
    "Xdn_train, Xdn_test, ydn_train, ydn_test = X_train3, X_test3, y_train0, y_test0\n",
    "Xd1_train,Xd1_test, yd1_train,  yd1_test = X_train1, X_test1, y_train1, y_test1\n",
    "\n",
    "nominal_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "nominal_dataset = nominal_dataset.batch(int(batch_size/2))\n",
    "domain1_dataset = tf.data.Dataset.from_tensor_slices((Xd1_train, yd1_train))\n",
    "domain1_dataset = domain1_dataset.batch(int(batch_size/2))\n",
    "training_dataset = tf.data.Dataset.zip((nominal_dataset, domain1_dataset))\n",
    "def _prepare_data_for_dann_training(nominal_data, domain1_data,\n",
    "                                    main_head_name='main_preds', domain1_head_name='domain_preds'):\n",
    "\n",
    "    nominal_images, nominal_labels = nominal_data\n",
    "    domain1_images, domain1_labels = domain1_data\n",
    "\n",
    "    num_nominal = tf.shape(nominal_images)[0]\n",
    "    num_domain1 = tf.shape(domain1_images)[0]\n",
    "\n",
    "    batch_images = tf.concat((nominal_images, domain1_images), axis=0)\n",
    "    batch_labels = tf.concat((nominal_labels, domain1_labels), axis=0)\n",
    "\n",
    "    # not to penalize the model for its prediscions on the domain images,\n",
    "    # by assigning a weight = 0 to these elements of the batch:\n",
    "    nominal_weight_per_sample = tf.tile([1.], [num_nominal])\n",
    "    domain1_weight_per_sample = tf.tile([0.], [num_domain1])\n",
    "    batch_sample_weights = tf.concat((nominal_weight_per_sample, domain1_weight_per_sample), axis=0)\n",
    "\n",
    "    # domain classifiscation\n",
    "    # we prepared ydn_train and yd1_train to be as what we need here\n",
    "    # but it is simpler to reuse batch_sample_weights that passing extra argument to the function\n",
    "    domain1_labels = batch_sample_weights\n",
    "    domain1_sample_weights = tf.tile([1.], [num_nominal + num_domain1])\n",
    "\n",
    "    batch_domain1 = {main_head_name: batch_labels,\n",
    "                     domain1_head_name: domain1_labels}\n",
    "    batch_sample_weights = {main_head_name: batch_sample_weights,\n",
    "                            domain1_head_name: domain1_sample_weights}\n",
    "\n",
    "    return batch_images, batch_domain1, batch_sample_weights\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "label_preds_head_name = 'label_preds'\n",
    "domain1_preds_head_name = 'domain1_preds'\n",
    "prepare_for_dann_training_fn = functools.partial(_prepare_data_for_dann_training,\n",
    "                                                 main_head_name=label_preds_head_name,\n",
    "                                                 domain1_head_name=domain1_preds_head_name)\n",
    "\n",
    "training_dataset = training_dataset.map(prepare_for_dann_training_fn, num_parallel_calls=4)\n",
    "print(training_dataset)\n",
    "\n",
    "\n",
    "# testing dataset\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices((Xd1_test, yd1_test))\n",
    "testing_dataset = testing_dataset.batch(batch_size)\n",
    "\n",
    "def _prepare_data_for_dann_testing(domain1_images, domain1_labels,\n",
    "                                      main_head_name='main_preds', domain1_head_name='domain_preds'):\n",
    "    # the batch contains only validation/test images from the target domain. \n",
    "    # this time, we want to evaluate the main loss over these images, so we assign a normal loss\n",
    "    # weight = 1 to each samples.\n",
    "    num_samples = tf.shape(domain1_images)[0]\n",
    "\n",
    "    # want to evaluate over\n",
    "    loss_weights = tf.tile([1], [num_samples])\n",
    "\n",
    "    # to assure we have labels as zeroes\n",
    "    domain1_labels = tf.tile([0], [num_samples])\n",
    "\n",
    "    batch_targets = {main_head_name: domain1_labels,\n",
    "                     domain1_head_name: domain1_labels}\n",
    "    batch_sample_weights = {main_head_name: loss_weights,\n",
    "                            domain1_head_name: loss_weights}\n",
    "\n",
    "    return domain1_images, batch_targets, batch_sample_weights\n",
    "\n",
    "\n",
    "prepare_for_dann_testing_fn = functools.partial(_prepare_data_for_dann_testing,\n",
    "                                                main_head_name=label_preds_head_name,\n",
    "                                                domain1_head_name=domain1_preds_head_name)\n",
    "\n",
    "testing_dataset = testing_dataset.map(prepare_for_dann_testing_fn, num_parallel_calls=4)\n",
    "print(testing_dataset)\n",
    "\n",
    "\n",
    "###\n",
    "# Build the model\n",
    "\n",
    "# build feature extractor layers and the label prediction model\n",
    "num_classes = 1 # 1 - probability to be of the target (additional b-jet) class\n",
    "#inputs = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "#fe_hiddens = []\n",
    "#for i in range(len(config['model']['hidden_layers'])):\n",
    "#    layer_name = 'feature_exctractor_{}'.format(i)\n",
    "#    if i==0:\n",
    "#        hidden_i = tf.keras.layers.Dense(\n",
    "#                units      = config['model']['hidden_layers'][i], \n",
    "#                activation = config['model']['act_func'][i],\n",
    "#                name       = layer_name\n",
    "#                )(inputs)\n",
    "#    else:\n",
    "#        hidden_i = tf.keras.layers.Dense(\n",
    "#                units      = config['model']['hidden_layers'][i],\n",
    "#                activation = config['model']['act_func'][i],\n",
    "#                name       = layer_name\n",
    "#                )(fe_hiddens[i-1])\n",
    "#    fe_hiddens.append(hidden_i)\n",
    "#\n",
    "#label_preds_head_name = 'label_preds'\n",
    "#label_preds = tf.keras.layers.Dense(\n",
    "#        units      = num_classes,\n",
    "#        activation = \"sigmoid\",\n",
    "#        name       = label_preds_head_name\n",
    "#        )(fe_hiddens[-1])\n",
    "\n",
    "inputs        = tf.keras.layers.Input(shape=(X_train.shape[1],))\n",
    "hidden1       = tf.keras.layers.Dense(30, activation='relu')(inputs)\n",
    "hidden2       = tf.keras.layers.Dense(15, activation='relu')(hidden1)\n",
    "hidden3       = tf.keras.layers.Dense(7, activation='relu')(hidden2)\n",
    "label_preds   = tf.keras.layers.Dense(num_classes, activation='softmax', name=label_preds_head_name)(hidden3)\n",
    "\n",
    "label_prediction_model = tf.keras.models.Model(\n",
    "        inputs  = inputs,\n",
    "        outputs = label_preds,\n",
    "        name    = 'classification_model')\n",
    "label_prediction_model.summary()\n",
    "\n",
    "label_prediction_model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                               loss='binary_crossentropy',\n",
    "                               metrics= ['AUC', 'accuracy'])\n",
    "\n",
    "from keras_custom_callbacks import SimpleLogCallback\n",
    "metrics_to_print_nom = collections.OrderedDict([(\"source-loss\", \"loss\"),\n",
    "                                            #(\"target-loss\", \"val_loss\"),\n",
    "                                            (\"source-acc\", \"accuracy\"),\n",
    "                                            #(\"target-acc\", \"val_acc\")\n",
    "])\n",
    "\n",
    "callbacks_nom = [\n",
    "        SimpleLogCallback(metrics_to_print_nom, num_epochs=num_epoch, log_frequency=1)\n",
    "]\n",
    "\n",
    "y_train = np.asarray(y_train).astype('int32').reshape((-1,1))\n",
    "y_test = np.asarray(y_test).astype('int32').reshape((-1,1))\n",
    "\n",
    "nom_fit = label_prediction_model.fit(x = X_train,\n",
    "                                     y = y_train,\n",
    "                                     epochs = num_epoch,\n",
    "                                     validation_data = (X_test, y_test),\n",
    "                                     callbacks = callbacks_nom)\n",
    "\n",
    "d1_as_nom_fit = label_prediction_model.fit(x = Xd1_train,\n",
    "                                           y = yd1_train,\n",
    "                                           epochs = num_epoch,\n",
    "                                           validation_data = (Xd1_test, yd1_test),\n",
    "                                           callbacks = callbacks_nom)\n",
    "\n",
    "\n",
    "@tf.custom_gradient\n",
    "def reverse_gradient(x, hp_lambda):\n",
    "    \"\"\"\n",
    "    Flips the sign of the incoming gradient during backpropagation.\n",
    "    :param x:           Input tensor\n",
    "    :param hp_lambda:   Hyper parameter lambda (DANN parameter)\n",
    "    :return:            Input tensor with reverse gradient (+ function to compute this reversed gradient)\n",
    "    \"\"\"\n",
    "\n",
    "    # Feed-forward operation:\n",
    "    y = tf.identity(x)\n",
    "\n",
    "    # Back-propagation/gradient-computing operation:\n",
    "    def _flip_gradient(dy):\n",
    "        # Since the decorated function 'reverse_gradient()' actually has 2 inputs\n",
    "        # (counting 'hp_lambda'), we have to return the gradient for each -- but\n",
    "        # anyway, the derivative wrt 'hp_lambda' is null:\n",
    "        return tf.math.negative(dy) * hp_lambda, tf.constant(0.)\n",
    "\n",
    "    return y, _flip_gradient\n",
    "\n",
    "\n",
    "# wrap the reverse gradient as a Keras layer\n",
    "class GradientReversal (tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Flip the sign of gradient during training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        return reverse_gradient(inputs, self.hp_lambda)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['hp_lambda'] = self.hp_lambda\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "\n",
    "# create domain classification model\n",
    "hp_lambda = tf.Variable(1.0)\n",
    "num_domains = 1 # 2: 'source' vs. 'target' (maybe not what we need), 1: prob. to be more or less 'source'-like\n",
    "\n",
    "domain1_preds_head_name = 'domain1_preds'\n",
    "#domain1_preds = GradientReversal(hp_lambda)(fe_hiddens[-1])\n",
    "#domain1_preds = tf.keras.layers.Dense(\n",
    "#        units      = 10,\n",
    "#        activation = 'relu',\n",
    "#        name       = 'domain1_dense'\n",
    "#        )(domain1_preds)\n",
    "#domain1_preds = tf.keras.layers.Dense(\n",
    "#        units      = num_domains,\n",
    "#        activation = \"sigmoid\",\n",
    "#        name       = domain1_preds_head_name\n",
    "#        )(domain1_preds)\n",
    "#\n",
    "#domain_classification_model1 = tf.keras.models.Model(\n",
    "#        inputs  = inputs,\n",
    "#        outputs = domain1_preds,\n",
    "#        name    = 'domain_dlassification_model')\n",
    "##domain_classification_model1.summary()\n",
    "\n",
    "domain1_preds = GradientReversal(hp_lambda)(hidden3)\n",
    "domain1_preds = tf.keras.layers.Dense(12, activation='linear')(domain1_preds)\n",
    "domain1_preds = tf.keras.layers.Dense(5, activation='linear', name=\"do5\")(domain1_preds)\n",
    "domain1_preds = tf.keras.layers.Activation(\"elu\", name=\"do6\")(domain1_preds)\n",
    "domain1_preds = tf.keras.layers.Dropout(0.5)(domain1_preds)\n",
    "domain1_preds = tf.keras.layers.Dense(num_domains, activation='softmax', name=domain_preds_head_name)(domain1_preds)\n",
    "domain_classification_model1 = tf.keras.models.Model(inputs=inputs, outputs=domain1_preds)\n",
    "\n",
    "\n",
    "# build a combined model\n",
    "combined_model = tf.keras.models.Model(inputs=inputs, outputs=[label_preds, domain1_preds])\n",
    "\n",
    "combined_model.compile(\n",
    "                       optimizer=tf.keras.optimizers.Adam(),\n",
    "                       loss={\n",
    "                           label_preds_head_name:   'binary_crossentropy',\n",
    "                           domain1_preds_head_name: 'binary_crossentropy'},\n",
    "                       loss_weights={\n",
    "                           label_preds_head_name:   1,\n",
    "                           domain1_preds_head_name: 1},\n",
    "                       metrics={ # weighted_metrics ?\n",
    "                           label_preds_head_name:    ['AUC', 'accuracy'],\n",
    "                           domain1_preds_head_name:  ['AUC', 'accuracy']}\n",
    "                       )\n",
    "\n",
    "combined_model.summary()\n",
    "\n",
    "\n",
    "# define some metrics\n",
    "metrics_to_print = collections.OrderedDict([\n",
    "    ('lc-loss', label_preds_head_name + '_loss'),\n",
    "    ('d-loss', domain1_preds_head_name + '_loss'),\n",
    "    ('lc-acc', label_preds_head_name + '_accuracy'),\n",
    "    ('d-acc', domain1_preds_head_name + '_accuracy'),\n",
    "    #('target c-acc', 'val_' + label_preds_head_name + '_acc')\n",
    "])\n",
    "\n",
    "callbacks = [\n",
    "        SimpleLogCallback(metrics_to_print, num_epochs=num_epoch, log_frequency=1)\n",
    "]\n",
    "\n",
    "\n",
    "# fit and save the model\n",
    "starting_time = time.time()\n",
    "if config['model']['new_or_load'] == 'new' :\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3)\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose = 1)\n",
    "\n",
    "    #the_fit = combined_model.fit(training_dataset,\n",
    "    #                             epochs = num_epoch,\n",
    "    #                             validation_data = testing_dataset,\n",
    "    #                             verbose = 0,\n",
    "    #                             #callbacks = [lr_callback, early_stopping, callbacks])\n",
    "    #                             callbacks = callbacks)\n",
    "    model.save(\"name\" + datetime.now().strftime('%d_%m_%Y_%H%M'))\n",
    "\n",
    "# evaluate the model\n",
    "loss_n, acc_n = model.evaluate(X_test, y_test.values, weighted_metrics=None)\n",
    "print('Measured model accuracy on the nominal dataset: {}, loss: {}'.format(acc_n, loss_n))\n",
    "loss_d1, acc_d1 = model.evaluate(Xd1_test, yd1_test, weighted_metrics=None)\n",
    "print('Measured model accuracy on the dom1 dataset: {}, loss: {}'.format(acc_d1, loss_d1))\n",
    "\n",
    "\n",
    "training_time = time.time() - starting_time\n",
    "print('Training time:i {}'.format(training_time))\n",
    "\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if phase == 'dann':\n",
    "    model, the_fit = train_DANN(lamda)\n",
    "    y_pred_test_class = model.predict(X_test_class).ravel()\n",
    "    y_pred_train_class = model.predict(X_train_class).ravel()\n",
    "    y_pred_test_domain = model.predict(X_test_domain).ravel()\n",
    "    y_pred_train_domain = model.predict(X_train_domain).ravel()\n",
    "    n_score_class = np.concatenate((y_pred_test_class.copy(),y_pred_train_class.copy()))\n",
    "    n_score_domain = np.concatenate((y_pred_test_domain.copy(),y_pred_train_domain.copy()))\n",
    "if phase == 'regular':\n",
    "    model = build_model(X_train,node)\n",
    "    the_fit = model.fit(X_train, y_train.values, epochs=num_epoch,\n",
    "                     validation_data=(X_test,y_test),#weights_test),\n",
    "                    #sample_weight=weights_train,\n",
    "                  #  batch_size = size,\n",
    "                    callbacks=[#SensitivitySpecificityCallback(),\n",
    "                               tf.keras.callbacks.LearningRateScheduler(lr_exp_decay),earlyStopping])\n",
    "    y_pred_test = model.predict(X_test).ravel()\n",
    "    y_pred_train = model.predict(X_train).ravel()\n",
    "    n_score = np.concatenate((y_pred_test.copy(),y_pred_train.copy()))\n",
    "    n_truth = pd.concat([y_test.copy(),y_train.copy()],ignore_index = True)\n",
    "\n",
    "training_time = time.time( ) - starting_time\n",
    "print(\"Training time:\",training_time)\n",
    "\n",
    "#model.summary()\n",
    "n_num = pd.concat([num_test.copy(),num_train.copy()],ignore_index = True)\n",
    "n_pt = pd.concat([pt_test.copy(),pt_train.copy()],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(X_train_class, y_train))\n",
    "print(model.score(X_test_class, y_test))\n",
    "print(model.score(X_train_domain, y_train))\n",
    "print(model.score(X_test_domain, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(the_fit,', All_var')\n",
    "print(\"test AUC score \"+str(roc_auc_score(y_true=y_test, y_score=y_pred_test)))\n",
    "#plot(dR_var[0],', dR_var')\n",
    "#plot(Mass_var[0],', Mass_var')\n",
    "#plot(all_var_pt[0],', All_var+pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#plot NN score distribution\n",
    "def plot_DNN(ii,source,truth0,shownumber):\n",
    "    bins = 50\n",
    "    source_s, source_b = [],[]\n",
    "    for i in range(len(source)):\n",
    "        if truth0[i] == 1:source_s.append(source[i])\n",
    "        else:source_b.append(source[i])\n",
    "    (n1, bins1, patches1)=plt.hist(source_s,bins = bins,density = True,histtype = 'stepfilled',alpha = 0.5,label = 'sig',color = 'c')\n",
    "    (n2, bins2, patches2)=plt.hist(source_b,bins = bins,density = True,histtype = 'stepfilled',alpha = 0.5,label = 'bkg',color = 'orange')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Density')\n",
    "    plt.xlabel('NN score')\n",
    "    #axs[0].set_ylim([0, 4])\n",
    "    #axs[0].set_xscale('log')\n",
    "    plt.legend()\n",
    "   # axs[1].set_ylim([0.4, 1.2])\n",
    "    plt.title(\"NN Score \"+\" Distribution of \"+ii)    \n",
    "    if shownumber == 1:\n",
    "        plt.show()\n",
    "    else:\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "        return np.mean(abs_ratio[0]),np.mean(abs_ratio[1])\n",
    "plot_DNN(\"Powheg + Pythia8 (nominal)\",n_score,n_truth,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_train_test(y_pred_train, y_train, y_pred_test, y_test, \n",
    "                   xlabel=\"NN Score\", title=\"NN\", \n",
    "                   weights_train=weights_train.values, weights_test=weights_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     27,
     60
    ]
   },
   "outputs": [],
   "source": [
    "#prepare train and test dataset\n",
    "def analyze_file_step1(name):\n",
    "    filename=(name)\n",
    "    file = ur.open(filename)\n",
    "    tree = file[\"ttbar\"]\n",
    "    fulldata = tree.arrays(library=\"pd\")\n",
    "    fulldata = fulldata.sample(frac=1).reset_index(drop=True)\n",
    "    target_eff = 4\n",
    "    if target_eff == 4: fulldata = fulldata[fulldata.bjet_number>3]  \n",
    "    else:   fulldata = fulldata[fulldata.bjet_number==3]  \n",
    "    weights,target,bjet_num,event,pt = extract(fulldata)\n",
    "    train_size = 0.75 # fraction of sample used for training\n",
    "    X_train, X_test, y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test= \\\n",
    "        train_test_split(fulldata, target, weights, event, pt, train_size=train_size)\n",
    "    y_train, y_test, weights_train, weights_test, num_train, num_test, pt_train, pt_test = \\\n",
    "        y_train.reset_index(drop=True),y_test.reset_index(drop=True), \\\n",
    "        weights_train.reset_index(drop=True), weights_test.reset_index(drop=True),\\\n",
    "        num_train.reset_index(drop=True),num_test.reset_index(drop=True),\\\n",
    "        pt_train.reset_index(drop=True),pt_test.reset_index(drop=True)\n",
    "    if name == \"3j3b.root\":\n",
    "        X_train_class,X_train_domain = set_zero(X_train)\n",
    "        X_test_class,X_test_domain = set_zero(X_test)\n",
    "        X_train_class,X_test_class = trans(X_train_class,X_test_class)\n",
    "        X_train_domain,X_test_domain = trans(X_train_domain,X_test_domain)\n",
    "        return X_train_class, X_test_class,X_train_domain, X_test_domain, y_train, y_test, num_train, num_test, pt_train, pt_test\n",
    "    else: \n",
    "        X_train,X_test = trans(X_train,X_test)\n",
    "        return X_train, X_test, y_train, y_test, num_train, num_test, pt_train, pt_test\n",
    "# train DANN with only one branch\n",
    "def analyze_file_step2(lamda,X_train0, X_test0, y_train0, y_test0, num_train0, num_test0,\n",
    "                       pt_train0, pt_test0,X_train1, X_test1, y_train1, y_test1, num_train1, \n",
    "                       num_test1, pt_train1, pt_test1):\n",
    "    auc,bias = [],[[],[]]\n",
    "    model, the_fit = train_DANN(lamda)\n",
    "    y_pred_test0 = model.predict(X_test0).ravel()\n",
    "    y_pred_train0 = model.predict(X_train0).ravel()\n",
    "    y_pred_test1 = model.predict(X_test1).ravel()\n",
    "    y_pred_train1 = model.predict(X_train1).ravel()\n",
    "    n_num0 = pd.concat([num_test0.copy(),num_train0.copy()],ignore_index = True)\n",
    "    n_num1 = pd.concat([num_test1.copy(),num_train1.copy()],ignore_index = True)\n",
    "    n_score0 = np.concatenate((y_pred_test0.copy(),y_pred_train0.copy()))\n",
    "    n_score1 = np.concatenate((y_pred_test1.copy(),y_pred_train1.copy()))\n",
    "    n_truth0 = pd.concat([y_test0.copy(),y_train0.copy()],ignore_index = True)\n",
    "    n_truth1 = pd.concat([y_test1.copy(),y_train1.copy()],ignore_index = True)\n",
    "    n_pt0 = pd.concat([pt_test0.copy(),pt_train0.copy()],ignore_index = True)\n",
    "    n_pt1 = pd.concat([pt_test1.copy(),pt_train1.copy()],ignore_index = True)\n",
    "    auc_test0 = roc_auc_score(y_true=y_test0, y_score=y_pred_test0)#,sample_weight=weights_test)\n",
    "    coll_all0 = compute_4eff_comb(n_truth0, n_score0, n_num0, n_pt0, 1)\n",
    "    auc_test1 = roc_auc_score(y_true=y_test1, y_score=y_pred_test1)#,sample_weight=weights_test)\n",
    "    coll_all1 = compute_4eff_comb(n_truth1, n_score1, n_num1, n_pt1, 1)\n",
    "    auc=[auc_test0,auc_test1]\n",
    "    n_score=[n_score0,n_score1]\n",
    "    n_truth=[n_truth0,n_truth1]\n",
    "    if target_eff == 4: plot_range = [0,1,2,3]\n",
    "    else:plot_range = [0,1,2]\n",
    "    for i in plot_range:\n",
    "        x,y,z = check3_pt_bias2(i,coll_all0)\n",
    "        bias[0].append(x)\n",
    "    for i in plot_range:\n",
    "        x,y,z = check3_pt_bias2(i,coll_all1)\n",
    "        bias[1].append(x)\n",
    "    return auc,bias,n_score,n_truth\n",
    "# train DANN with multiple branches but failed\n",
    "def analyze_file_step3(lamda,X_train0, X_test0,X_train3, X_test3, y_train0, y_test0, num_train0, num_test0,\n",
    "                       pt_train0, pt_test0,X_train1, X_test1, y_train1, y_test1, num_train1, num_test1, pt_train1, \n",
    "                       pt_test1,X_train2, X_test2, y_train2, y_test2, num_train2, num_test2, pt_train2, pt_test2):\n",
    "    auc,bias = [],[[],[]]\n",
    "    model, the_fit = train_DANN2(lamda)\n",
    "    \n",
    "    y_pred_test0 = model.predict(X_test0).ravel()\n",
    "    y_pred_train0 = model.predict(X_train0).ravel()\n",
    "    y_pred_test1 = model.predict(X_test1).ravel()\n",
    "    y_pred_train1 = model.predict(X_train1).ravel()\n",
    "    y_pred_test2 = model.predict(X_test2).ravel()\n",
    "    y_pred_train2 = model.predict(X_train2).ravel()\n",
    "    y_pred_test3 = model.predict(X_test3).ravel()\n",
    "    y_pred_train3 = model.predict(X_train3).ravel()\n",
    "    \n",
    "    n_num0 = pd.concat([num_test0.copy(),num_train0.copy()],ignore_index = True)\n",
    "    n_num1 = pd.concat([num_test1.copy(),num_train1.copy()],ignore_index = True)\n",
    "    n_num2 = pd.concat([num_test2.copy(),num_train2.copy()],ignore_index = True)\n",
    "    \n",
    "    n_score0 = np.concatenate((y_pred_test0.copy(),y_pred_train0.copy()))\n",
    "    n_score1 = np.concatenate((y_pred_test1.copy(),y_pred_train1.copy()))\n",
    "    n_score2 = np.concatenate((y_pred_test2.copy(),y_pred_train2.copy()))\n",
    "    n_score3 = np.concatenate((y_pred_test3.copy(),y_pred_train3.copy()))\n",
    "    \n",
    "    n_truth0 = pd.concat([y_test0.copy(),y_train0.copy()],ignore_index = True)\n",
    "    n_truth1 = pd.concat([y_test1.copy(),y_train1.copy()],ignore_index = True)\n",
    "    n_truth2 = pd.concat([y_test2.copy(),y_train2.copy()],ignore_index = True)\n",
    "    \n",
    "    n_pt0 = pd.concat([pt_test0.copy(),pt_train0.copy()],ignore_index = True)\n",
    "    n_pt1 = pd.concat([pt_test1.copy(),pt_train1.copy()],ignore_index = True)\n",
    "    n_pt2 = pd.concat([pt_test2.copy(),pt_train2.copy()],ignore_index = True)\n",
    "    \n",
    "    auc_test0 = roc_auc_score(y_true=y_test0, y_score=y_pred_test0)#,sample_weight=weights_test)\n",
    "    coll_all0 = compute_4eff_comb(n_truth0, n_score0, n_num0, n_pt0, 1)\n",
    "    auc_test1 = roc_auc_score(y_true=y_test1, y_score=y_pred_test1)#,sample_weight=weights_test)\n",
    "    coll_all1 = compute_4eff_comb(n_truth1, n_score1, n_num1, n_pt1, 1)\n",
    "    auc_test2 = roc_auc_score(y_true=y_test2, y_score=y_pred_test2)#,sample_weight=weights_test)\n",
    "    coll_all2 = compute_4eff_comb(n_truth2, n_score2, n_num2, n_pt2, 1)\n",
    "    auc_test3 = roc_auc_score(y_true=y_test0, y_score=y_pred_test3)#,sample_weight=weights_test)\n",
    "    coll_all3 = compute_4eff_comb(n_truth0, n_score3, n_num0, n_pt0, 1)\n",
    "    coll_all = [coll_all0,coll_all1,coll_all2,coll_all3]\n",
    "    auc=[auc_test0,auc_test1,auc_test2,auc_test3]\n",
    "    n_score=[n_score0,n_score1,n_score2,n_score3]\n",
    "    n_truth=[n_truth0,n_truth1,n_truth2,n_truth0]\n",
    "    if target_eff == 4: plot_range = [0,1,2,3]\n",
    "    else:plot_range = [0,1,2]\n",
    "    for j in range(len(n_score)):\n",
    "        for i in plot_range:\n",
    "            x,y,z = check3_pt_bias2(i,coll_all[j])\n",
    "            bias[j].append(x)\n",
    "    return auc,bias,n_score,n_truth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lamda_list=np.linspace(0,50,num=26)\n",
    "lamda_list=[[0,0,0]]\n",
    "X_train0, X_test0,X_train3, X_test3, y_train0, y_test0, num_train0, num_test0, pt_train0, pt_test0=analyze_file_step1(\"3j3b.root\")\n",
    "X_train1, X_test1, y_train1, y_test1, num_train1, num_test1, pt_train1, pt_test1=analyze_file_step1(\"3j3b_sherpa.root\")\n",
    "#Pythia8_&_aMC@NLO+Pythia8：set3\n",
    "X_train2, X_test2, y_train2, y_test2, num_train2, num_test2, pt_train2, pt_test2=analyze_file_step1(\"3j3b_set3.root\")\n",
    "i=0\n",
    "num = len(lamda_list)\n",
    "auc,bias,n_score,n_truth = [0 for _ in range(num)],[0 for _ in range(num)],[0 for _ in range(num)],[0 for _ in range(num)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lamda_list[i])\n",
    "#auc[i],bias[i],n_score[i],n_truth[i]=analyze_file_step2(lamda_list[i],X_train0, X_test0, y_train0, y_test0, num_train0, num_test0,\n",
    "#                       pt_train0, pt_test0,X_train1, X_test1, y_train1, y_test1, num_train1, num_test1, pt_train1, pt_test1)\n",
    "auc[i],bias[i],n_score[i],n_truth[i]=analyze_file_step3(lamda_list[i],X_train0, X_test0,X_train3, X_test3, y_train0, y_test0, num_train0, num_test0,\n",
    "                       pt_train0, pt_test0,X_train1, X_test1, y_train1, y_test1, num_train1, num_test1, pt_train1, pt_test1,X_train2, X_test2, y_train2, y_test2, num_train2, num_test2, pt_train2, pt_test2)\n",
    "i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_NN(ii,source,target,truth0,truth1,shownumber):\n",
    "    bins = 50\n",
    "    source_s, source_b, target_s, target_b = [],[],[],[]\n",
    "    for i in range(len(source)):\n",
    "        if truth0[i] == 1:source_s.append(source[i])\n",
    "        else:source_b.append(source[i])\n",
    "    for i in range(len(target)):            \n",
    "        if truth1[i] == 1:target_s.append(target[i])\n",
    "        else:target_b.append(target[i])\n",
    "    fig, axs = plt.subplots(2,1, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    (n1, bins1, patches1)=axs[0].hist(source_s,bins = bins,density = True,histtype = 'step',label = 'class_sig',color = 'c')\n",
    "    (n2, bins2, patches2)=axs[0].hist(source_b,bins = bins,density = True,histtype = 'step',label = 'class_bkg',color = 'orange')\n",
    "    (n3, bins3, patches3)=axs[0].hist(target_s,bins = bins,density = True,histtype = 'stepfilled',alpha = 0.5,label = 'domain_sig',color = 'c')\n",
    "    (n4, bins4, patches4)=axs[0].hist(target_b,bins = bins,density = True,histtype = 'stepfilled',alpha = 0.5,label = 'domain_bkg',color = 'orange')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel('Density')\n",
    "    axs[0].set_ylim([0, 4])\n",
    "    #axs[0].set_xscale('log')\n",
    "    axs[0].set(xticklabels=[])\n",
    "    n = [n1,n2,n3,n4]\n",
    "    div,abs_ratio = [[],[]],[[],[]]\n",
    "    for i in range(len(n1)):\n",
    "        div[0].append(n1[i]/n3[i])\n",
    "        abs_ratio[0].append(abs(n1[i]/n3[i]-1))\n",
    "    for i in range(len(n2)):\n",
    "        div[1].append(n2[i]/n4[i])\n",
    "        abs_ratio[1].append(abs(n2[i]/n4[i]-1))\n",
    "    axs[1].plot(div[0],label = 'deviation_sig',color = 'c')\n",
    "    axs[1].plot(div[1],label = 'deviation_bkg',color = 'orange')\n",
    "    axs[1].hlines(y = 1, xmin = 0, xmax = 50,color = 'r',linestyles='dashed',label='truth')\n",
    "    axs[1].set_xlabel(\"Classifier Score\")\n",
    "    axs[1].set_ylabel('Class/Domain')\n",
    "    axs[1].legend()\n",
    "   # axs[1].set_ylim([0.4, 1.2])\n",
    "    axs[0].title.set_text(\"NN Score \"+\" Distribution with \\lambda = \"+str(ii))    \n",
    "    if shownumber == 1:\n",
    "        plt.show()\n",
    "        print(\"mean ratio deviation of signal: \"+str(round(np.mean(abs_ratio[0]),4)))\n",
    "        print(\"mean ratio deviation of background: \"+str(round(np.mean(abs_ratio[1]),4)))\n",
    "    else:\n",
    "        fig.clear()\n",
    "        plt.close(fig)\n",
    "        return np.mean(abs_ratio[0]),np.mean(abs_ratio[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_dev,bkg_dev = [0 for _ in range(num)],[0 for _ in range(num)]\n",
    "for i in range(num):sig_dev[i],bkg_dev[i]= plot_NN(lamda_list[i],n_score[i][0],n_score[i][1],n_truth[i][0],n_truth[i][1],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in [0,1,3,25]:plot_NN(lamda_list[i],n_score[i][0],n_score[i][1],n_truth[i][0],n_truth[i][1],1)\n",
    "plot_NN(0,n_score[0][0],n_score[0][1],n_truth[0][0],n_truth[0][1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lamda_list,sig_dev,color = 'c',label = 'ratio deviation of signal')\n",
    "plt.plot(lamda_list,bkg_dev,color = 'orange',label = 'ratio deviation of background')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('ratio Deviation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "auc1 = trans_shape(auc)\n",
    "plt.plot(lamda_list,auc1[0],color = 'r',label = 'Class')\n",
    "plt.plot(lamda_list,auc1[1],color = 'b',label = 'Domain')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_eff(n_score):\n",
    "    coll_all = compute_4eff_comb(n_truth, n_score, n_num, n_pt, 1)\n",
    "    print(\"Efficiency of Leading pT b-jet from tops:\", round(coll_all[0][0],3))\n",
    "    print(\"Efficiency of sub-Leading pT b-jet from tops:\", round(coll_all[0][1],3))\n",
    "    print(\"Efficiency of Leading pT b-jet additional:\", round(coll_all[0][2],3))\n",
    "    print(\"Efficiency of sub-Leading pT b-jet additional:\", round(coll_all[0][3],3))\n",
    "    print('both from top are assigned correctly:', round(coll_all[0][5],3))\n",
    "    return coll_all\n",
    "coll_class = print_eff(n_score_class)\n",
    "coll_domain = print_eff(n_score_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [0,1,2,3]:\n",
    "    check3_pt_bias2(i,coll_class)\n",
    "    #check3_pt_bias2(i,coll_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr(the_fit):\n",
    "    plt.plot(the_fit.history['val_lr'],label='test_lr')\n",
    "    plt.plot(the_fit.history['lr'],label='train_lr')\n",
    "    plt.title('learning rate schedule')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.ylabel('learning rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_lr(the_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#when three b-jet under cut, consider the highest score one as additional\n",
    "def compute_4eff_3b4b(dataset, NNscore,event,n_pt,cut):\n",
    "    copy = event.copy()\n",
    "    event_number = len(set(copy))\n",
    "    bjetpt_model = [[] for _ in range(4)] #lead_top, sub_top, lead_add, sub_add\n",
    "    bjetpt_truth = [[] for _ in range(4)]\n",
    "    n = [[0 for _ in range(4)],[0 for _ in range(4)]]\n",
    "    lead_top, sub_top,lead_add, sub_add=[0,0],[0,0],[0,0],[0,0]\n",
    "    tot_cut, tot_cor, both_top_cor = [0,0],[0,0],[0,0]\n",
    "    #coll = [[] for _ in range(event_number)]\n",
    "    coll = [[] for _ in range(max(set(copy)))]\n",
    "    for i in range(len(dataset)):\n",
    "        coll[int(event[i])-1].append([NNscore[i], dataset[i], n_pt[i]])\n",
    "    n3,n4=0,0    \n",
    "   # for i in range(event_number):\n",
    "    for i in range(max(set(copy))):\n",
    "        score_top = []\n",
    "        index_top = []\n",
    "        index_add = []\n",
    "        score_add = []\n",
    "        score = []\n",
    "        pt = []\n",
    "        index_0 = []\n",
    "        index_1 = []\n",
    "        pt_top =[]\n",
    "        pt_add = []\n",
    "        num_bjet = len(coll[i])\n",
    "        #print(num_bjet)\n",
    "        if num_bjet == 3:\n",
    "            bcase = 0\n",
    "            n3 += 1\n",
    "        else: \n",
    "            bcase = 1\n",
    "            n4 += 1\n",
    "        for j in range(num_bjet):\n",
    "            score.append(coll[i][j][0])\n",
    "            pt.append(coll[i][j][2])\n",
    "            if coll[i][j][1] == 0: \n",
    "                pt_top.append(coll[i][j][2])\n",
    "                index_0.append(j)\n",
    "            else: \n",
    "                pt_add.append(coll[i][j][2])\n",
    "                index_1.append(j)\n",
    "            if coll[i][j][0] <= cut: \n",
    "                score_top.append(coll[i][j][0])\n",
    "            else: score_add.append(coll[i][j][0])\n",
    "        \n",
    "        # tops by NN algorithm\n",
    "        if len(score_top) == 0:\n",
    "            fir_top_NN = -1\n",
    "            sec_top_NN = -1\n",
    "        elif len(score_top) == 1:\n",
    "            fir_top_NN = score.index(score_top[0])\n",
    "            sec_top_NN = -1\n",
    "            index_top.append(fir_top_NN)\n",
    "        else:\n",
    "            for z in range(2):\n",
    "                mini = min(score_top)\n",
    "                index_top.append(score.index(mini))\n",
    "                score_top.remove(mini)\n",
    "            if len(score_top) > 0:\n",
    "                for z in score_top:\n",
    "                    score_add.append(z)  \n",
    "            if pt[index_top[0]] >= pt[index_top[1]]:\n",
    "                fir_top_NN = index_top[0]\n",
    "                sec_top_NN = index_top[1]\n",
    "            else:\n",
    "                fir_top_NN = index_top[1]\n",
    "                sec_top_NN = index_top[0]\n",
    "       \n",
    "        # tops by truth information\n",
    "        pt_top.sort()\n",
    "        if len(pt_top) > 0: \n",
    "            fir_top_truth = pt.index(pt_top[-1])\n",
    "            if bcase == 0:\n",
    "                bjetpt_truth[0].append(pt_top[-1]/1000)\n",
    "        else: fir_top_truth = -1\n",
    "        if len(pt_top) > 1: \n",
    "            sec_top_truth = pt.index(pt_top[-2])\n",
    "            if bcase == 0:\n",
    "                bjetpt_truth[1].append(pt_top[-2]/1000)\n",
    "        else: sec_top_truth = -1\n",
    "        \n",
    "        \n",
    "        # add by NN algorithm\n",
    "        if len(score_add) == 0:\n",
    "            fir_add_NN = -1\n",
    "            sec_add_NN = -1\n",
    "        elif len(score_add) == 1:\n",
    "            fir_add_NN = score.index(score_add[0])\n",
    "            sec_add_NN = -1\n",
    "            index_add.append(fir_add_NN)\n",
    "        else:\n",
    "            ptadd = []\n",
    "            for z in score_add:\n",
    "                ptadd.append(coll[i][score.index(z)][2])\n",
    "                index_add.append(score.index(z))\n",
    "            ptadd.sort()\n",
    "            fir_add_NN = pt.index(ptadd[-1])\n",
    "            sec_add_NN = pt.index(ptadd[-2])\n",
    "\n",
    "        # add by truth information\n",
    "            \n",
    "        pt_add.sort()\n",
    "        if len(pt_add) > 0: \n",
    "            fir_add_truth = pt.index(pt_add[-1])\n",
    "            if bcase == 0:\n",
    "                bjetpt_truth[2].append(pt_add[-1]/1000)\n",
    "        else: fir_add_truth = -1\n",
    "        if len(pt_add) > 1: \n",
    "            sec_add_truth = pt.index(pt_add[-2])\n",
    "            if bcase == 0:\n",
    "                bjetpt_truth[3].append(pt_add[-2]/1000)\n",
    "        else: sec_add_truth = -1\n",
    "        \n",
    "        #compute eff of cut\n",
    "        x = len(common_member(index_add, index_1))\n",
    "        y = len(common_member(index_top, index_0))\n",
    "        tot_cut[bcase] += len(index_1)+len(index_0)\n",
    "        tot_cor[bcase] += x + y\n",
    "        if y == 2: both_top_cor[bcase] += 1\n",
    "        \n",
    "        #compare NN and truth\n",
    "        if fir_top_NN != -1:\n",
    "            n[bcase][0] += 1\n",
    "            if bcase == 0:\n",
    "                bjetpt_model[0].append(pt[fir_top_NN]/1000)\n",
    "            if fir_top_NN == fir_top_truth: lead_top[bcase] += 1\n",
    "        if sec_top_NN != -1:\n",
    "            n[bcase][1] += 1\n",
    "            if bcase == 0:\n",
    "                bjetpt_model[1].append(pt[sec_top_NN]/1000)\n",
    "            if sec_top_NN == sec_top_truth: sub_top[bcase] += 1\n",
    "        if fir_add_NN != -1:\n",
    "            n[bcase][2] += 1\n",
    "            if bcase == 0:\n",
    "                bjetpt_model[2].append(pt[fir_add_NN]/1000)\n",
    "            if fir_add_NN == fir_add_truth: lead_add[bcase] += 1\n",
    "        if sec_add_NN != -1:\n",
    "            n[bcase][3] += 1\n",
    "            if bcase == 0:\n",
    "                bjetpt_model[3].append(pt[sec_add_NN]/1000)\n",
    "            if sec_add_NN == sec_add_truth: sub_add[bcase] += 1\n",
    "    eff = [lead_top[0]/n[0][0], sub_top[0]/n[0][1],lead_add[0]/n[0][2],0,tot_cor[0]/tot_cut[0], both_top_cor[0]/n3]\n",
    "          # [lead_top[1]/n[1][0], sub_top[1]/n[1][1],lead_add[1]/n[1][2],sub_add[1]/n[1][3],tot_cor[1]/tot_cut[1], both_top_cor[1]/n4]]\n",
    "    correct_num = [eff, bjetpt_model, bjetpt_truth]\n",
    "    return correct_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_eff_4b(n,cut):\n",
    "    collection42 = compute_4eff_comb(n[1][0], n[1][1], n[1][2], n[1][3], cut)\n",
    "    print(\"Efficiency of Leading pT b-jet from tops:\", collection42[0][0])\n",
    "    print(\"Efficiency of sub-Leading pT b-jet from tops:\", collection42[0][1])\n",
    "    print(\"Efficiency of Leading pT b-jet additional:\", collection42[0][2])\n",
    "    print(\"Efficiency of sub-Leading pT b-jet additional:\", collection42[0][3])\n",
    "    print('both from top are assigned correctly:', collection42[0][5])\n",
    "    print('combinal eff by cut:', collection42[0][4])\n",
    "    return collection42\n",
    "def print_eff_3b(n,cut):\n",
    "    collection_3b4b = compute_4eff_3b4b(n[1][0], n[1][1], n[1][2], n[1][3], cut)\n",
    "    print(\"Efficiency of Leading pT b-jet from tops:\", collection_3b4b[0][0])\n",
    "    print(\"Efficiency of sub-Leading pT b-jet from tops:\", collection_3b4b[0][1])\n",
    "    print(\"Efficiency of Leading pT b-jet additional:\", collection_3b4b[0][2])\n",
    "    print(\"Efficiency of sub-Leading pT b-jet additional:\", collection_3b4b[0][3])\n",
    "    print('both from top are assigned correctly:', collection_3b4b[0][5])\n",
    "    print('combinal eff by cut:', collection_3b4b[0][4],\"\\n\")\n",
    "    return collection_3b4b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_eff == 3:\n",
    "    coll_all = print_eff_3b(all_var,1)\n",
    "    coll_dR =  print_eff_3b(dR_var,1)\n",
    "    coll_mass = print_eff_3b(Mass_var,1)\n",
    "    coll_pt = print_eff_3b(all_var_pt,1)\n",
    "if target_eff == 4:\n",
    "    coll_all = print_eff_4b(all_var,1)\n",
    "    coll_dR =  print_eff_4b(dR_var,1)\n",
    "    coll_mass = print_eff_4b(Mass_var,1)\n",
    "    coll_pt = print_eff_4b(all_var_pt,1)\n",
    "coll = [coll_all,coll_dR,coll_mass,coll_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "colors = ['r','g','b','y','c']\n",
    "labels = ['truth','All_var model','dR model','Mass model','All_var + pt model']\n",
    "def check3_pt_bias(x):\n",
    "    bins=[[25,60,90,120,160,400],[25,40,60,400],[25,45,70,100,160,400],[25,35,55,400]]\n",
    "    name = ['lead b-top','sub-lead b-top','lead b-Add','sub-lead b-Add']\n",
    "    biahist=[coll_all[2][x],coll_all[1][x],coll_dR[1][x],coll_mass[1][x],coll_pt[1][x]]\n",
    "    fig, axs = plt.subplots(2,1, gridspec_kw={'height_ratios': [2, 1]})\n",
    "    (n, bins, patches)=axs[0].hist(biahist, bins[x],color=colors, range =[0,400], \n",
    "                            density=True, histtype='step',label=labels)\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylabel('density')\n",
    "    axs[0].set_yscale('log')\n",
    "    #axs[0].set_xscale('log')\n",
    "    axs[0].set(xticklabels=[])\n",
    "    ratio,div,wdiv = [[] for i in range(len(labels)-1)],[0 for i in range(len(labels)-1)],[0 for i in range(len(labels)-1)]\n",
    "    for i in range(len(n[0])):\n",
    "        for j in range(len(coll)):\n",
    "            ratio[j].append(n[j+1][i]/n[0][i])\n",
    "            div[j] += (ratio[j][i]-1)**2\n",
    "            wdiv[j] += ((ratio[j][i]-1)*n[0][i]/(bins[i+1]-bins[i]))**2\n",
    "            axs[1].hlines(y = ratio[j][i], xmin = bins[i], xmax = bins[i+1],color=colors[j+1],label=labels[j+1])\n",
    "    axs[1].hlines(y = 1, xmin = bins[0], xmax = bins[-1],color = 'r',linestyles='dashed',label='truth')\n",
    "    axs[1].set_xlabel('Pt ('+name[x]+'), GeV')\n",
    "    axs[1].set_ylabel('model / truth')\n",
    "    #axs[1].set_xscale('log')\n",
    "    #axs[1].set_xticks(bins[x], bins[x],fontsize=7)\n",
    "    plt.show()\n",
    "    print(\"deviation of All_var model:\",np.sqrt(div[0]))\n",
    "    print(\"deviation of dR model:\",np.sqrt(div[1]))\n",
    "    print(\"deviation of Mass model:\",np.sqrt(div[2]))\n",
    "    print(\"deviation of All_var+pt model:\",np.sqrt(div[3]),\"\\n\")\n",
    "    print(\"weighted deviation of All_var model:\",1e5*np.sqrt(wdiv[0]))\n",
    "    print(\"weighted deviation of dR model:\",1e5*np.sqrt(wdiv[1]))\n",
    "    print(\"weighted deviation of Mass model:\",1e5*np.sqrt(wdiv[2]))\n",
    "    print(\"weighted deviation of All_var+pt model:\",1e5*np.sqrt(wdiv[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_xticks = ['Both b FT','Lead Pt, b FT','sub-Lead Pt, b FT','Lead Pt, add. b','sub-Lead Pt, add. b']\n",
    "xx = [0.2*i for i in range(5)]\n",
    "for i in [-1,0,1,2]:\n",
    "    plt.axhline(y = coll[i+1][0][3], xmin = 4/5, xmax = 1,color=colors[i+2],label=labels[i+2])\n",
    "    for j in range(len(coll)):\n",
    "        plt.axhline(y = coll[j][0][i], xmin = (i+1)/5, xmax = (i+2)/5,color=colors[j+1])\n",
    "ax = plt.gca()\n",
    "#ax.get_xaxis().set_visible(False)\n",
    "plt.yticks(np.arange(0, 1, 0.1))\n",
    "plt.xticks(xx, my_xticks,fontsize=7)\n",
    "plt.grid()\n",
    "plt.title('B-jet origin identification Efficiency in Pt order')\n",
    "plt.ylabel('Efficiency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_xticks = ['Both b FT','Lead Pt, b FT','sub-Lead Pt, b FT','Lead Pt, add. b','sub-Lead Pt, add. b']\n",
    "xx = [0.2*i for i in range(5)]\n",
    "def plot3b4b(collection,bcase,name):\n",
    "    for i in [-1,0,1,2]:\n",
    "        plt.axhline(y = collection[0][bcase][i], xmin = (i+1)/5, xmax = (i+2)/5,color='r')\n",
    "    plt.axhline(y = collection[0][bcase][3], xmin = 4/5, xmax = 1,color='r',label='reco level')\n",
    "    plt.yticks(np.arange(0, 1, 0.1))\n",
    "    plt.grid()\n",
    "    plt.xticks(xx, my_xticks,fontsize=7)\n",
    "    plt.title('B-jet origin identification Efficiency in Pt order ('+name+')')\n",
    "    plt.ylabel('Efficiency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot3b4b(collection_3b4b_14,0,'== 3b in All_var model')\n",
    "plot3b4b(collection_3b4b_14,1,'>= 4b in All_var model')\n",
    "plot3b4b(collection_3b4b_8,0,'== 3b in dR model')\n",
    "plot3b4b(collection_3b4b_8,1,'>= 4b in dR model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut_candidate = np.linspace(0.01, 1, 100)\n",
    "cut_candidate = np.linspace(0.1, 1, 91)\n",
    "coll2, cut_eff_com2= [],[]\n",
    "for i in cut_candidate:\n",
    " #   x = compute_4eff_cut(n_truth, n_score, n_num, n_pt, i)\n",
    "    y = compute_4eff_comb(n_truth, n_score2, n_num, n_pt, i)\n",
    " #   coll1.append(x)\n",
    "    coll2.append(y)\n",
    " #   cut_eff_top1.append(x[4])\n",
    " #   cut_eff_top2.append(y[4])\n",
    " #   cut_eff_add1.append(x[5])\n",
    " #   cut_eff_add2.append(y[5])\n",
    " #   cut_eff_com1.append(x[6])\n",
    "    cut_eff_com2.append(y[6])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(cut_candidate,cut_eff_add2,label='add efficiency',color='red')\n",
    "#plt.plot(cut_candidate,cut_eff_top2,label='top efficiency',color='blue')\n",
    "plt.plot(cut_candidate,cut_eff_com2,label='combination efficiency',color='green')\n",
    "plt.xlabel(\"optimal cut\")\n",
    "plt.ylabel(\"Efficiency of cut\")\n",
    "plt.title('no more than 2 b-jet assigned to tops under cut')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#print('maximum top efficiency/blue:', max(cut_eff_top2), 'when cut =', cut_candidate[cut_eff_top2.index(max(cut_eff_top2))])\n",
    "#print('maximum add efficiency/red:', max(cut_eff_add2), 'when cut =', cut_candidate[cut_eff_add2.index(max(cut_eff_add2))])\n",
    "print('maximum combination efficiency/green:', max(cut_eff_com2), 'when cut =', cut_candidate[cut_eff_com2.index(max(cut_eff_com2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr,_ = roc_curve(y_true=y_test, y_score=y_pred_test,sample_weight=weights_test)\n",
    "fpr2,tpr2,_ = roc_curve(y_true=y_train, y_score=y_pred_train,sample_weight=weights_train)\n",
    "plt.plot(fpr, tpr, color='blue',lw=2,label='test')\n",
    "plt.plot(fpr2, tpr2, color='red',lw=2,label='train')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.title('Receiver Operating Characteristic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_pred_test_sig = [weights_test[(y_test ==1) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test ==0) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0,1,num=50)]\n",
    "vamsasimov = [amsasimov(sumsig,sumbkg) for (sumsig,sumbkg) in zip(int_pred_test_sig,int_pred_test_bkg)]\n",
    "Z = max(vamsasimov)\n",
    "print(\"Z:\",Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1,num=50),vamsasimov, label='Significance (Z = {})'.format(np.round(Z,decimals=2)))\n",
    "\n",
    "\n",
    "plt.title(act_f + \" NN Significance\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"Significance_xgb.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num1 = 21\n",
    "coll_lambda = np.linspace(0.5,0.7,num=num1)\n",
    "print(coll_lambda)\n",
    "auc = [0 for i in range(num1)]\n",
    "bias,slope,chi2 = auc.copy(),auc.copy(),auc.copy()\n",
    "z=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(coll_lambda[z])\n",
    "auc[z],bias[z],slope[z],chi2[z]= scan_lambda(coll_lambda[z],X_train_class,X_train_domain,X_test_class,X_test_domain, \n",
    "        y_train, y_test, num_train, num_test, pt_train, pt_test)\n",
    "z += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trans_shape(data):\n",
    "    result=[[] for _ in range(len(data[0]))]\n",
    "    for i in range(len(data[0])):\n",
    "        for j in range(len(data)):\n",
    "            result[i].append(data[j][i])\n",
    "    return result\n",
    "def plot_scan_lambda(auc,bias,slope,chi2):\n",
    "    name = [\"1st FT\",'2nd FT','1st Add','2nd Add']\n",
    "    colors=['r','orange','b','c']\n",
    "    auc = trans_shape(auc)\n",
    "    bias = trans_shape(bias)\n",
    "    slope =trans_shape(slope)\n",
    "    chi2=trans_shape(chi2)\n",
    "    for i in range(4):\n",
    "        plt.plot(coll_lambda,bias[i],color=colors[i],label=name[i])\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"Weighted Bias\")\n",
    "    plt.title(\"At least four b-jets (None vs pT)\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "   # plt.plot(coll_lambda,auc[2],label='train mode')\n",
    "   # plt.plot(coll_lambda,auc[0],label='test mode')\n",
    "   # plt.ylabel(\"AUC\")\n",
    "   # plt.xlabel(\"lambda\")\n",
    "   # plt.legend()\n",
    "  #  plt.show()\n",
    "    for i in range(4):\n",
    "        plt.plot(coll_lambda,slope[i],color=colors[i],label=name[i])\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"slope\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    for i in range(4):\n",
    "        plt.plot(coll_lambda,chi2[i],color=colors[i],label=name[i])\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"chi2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#plot_scan_lambda(auc,bias,slope,chi2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
